# RKNN NPU Integration Plan

## Overview

This document tracks the integration of Rockchip NPU (Neural Processing Unit) support into the Avalanche infrastructure. The goal is to enable hardware-accelerated ML inference on Orange Pi 5 Plus nodes (opi01-03) via the RKNN Toolkit.

## Context

- **Hardware**: Orange Pi 5 Plus (RK3588 SoC with integrated NPU)
- **Nodes affected**: opi01-03 (K3s worker nodes)
- **Reference project**: [EZRKNN-Toolkit2](https://github.com/Pelochus/EZRKNN-Toolkit2)
- **Primary use case**: Edge ML inference (computer vision, real-time processing)

## Architecture Approach

- Use **NixOS modules** (not personalities) for flexibility
- Integrate into `nixos/profiles/role-k3s-worker.nix` or create a variant
- Enable opt-in NPU access for Kubernetes workloads
- No immediate use cases; foundation for future development

## Progress Status

### ‚úÖ Phase 1: COMPLETE (2025-11-15)

All foundation work is complete. Code is integrated and active on Orange Pi 5 Plus nodes.

**Completed:**
- [x] Design Architecture
- [x] Investigation & Findings - RKNPU not in linux_6_17, but found in linux 6.18-rc5
- [x] Create `nixos/pkgs/rknn/runtime.nix` - librknnrt.so + headers from EZRKNN-Toolkit2
- [x] Create `nixos/pkgs/rknn/toolkit-lite.nix` - Python 3.12 wheel + dependencies
- [x] Create `nixos/pkgs/rknn/default.nix` - Meta-package
- [x] Create `nixos/modules/nixos/rknn.nix` - Full NixOS module with:
  - Conditional enable flag (enabled for Orange Pi hosts)
  - Selective component installation
  - Device permissions via udev rules (for /dev/accel/accel0)
  - LD_LIBRARY_PATH configuration for librknnrt.so
  - aarch64-only guard
- [x] Create `nixos/overlays/rknn-packages.nix` - Package overlay
- [x] Import overlay and module into NixOS configuration

**Status**: ‚úÖ Code is active and integrated into Orange Pi 5 Plus hardware profile.

### ‚úÖ Phase 2: COMPLETE (2025-12-10)

**Status**: Kernel 6.18 is deployed and NPU hardware is working!

**Deployment Results** (2025-12-10):
- Deployed linux 6.18.0 to opi01-03 (Orange Pi 5 Plus nodes)
- **`rocket` driver loaded successfully** ‚úÖ
- All **3 NPU cores detected** and initialized:
  ```
  [   16.974889] rocket fdab0000.npu: Rockchip NPU core 0 version: 1179210309
  [   16.991713] rocket fdac0000.npu: Rockchip NPU core 1 version: 1179210309
  [   17.005136] rocket fdad0000.npu: Rockchip NPU core 2 version: 1179210309
  ```
- NPU device exposed via DRM accelerator framework: `/dev/accel/accel0` ‚úÖ
- Device permissions configured (mode 0666, group render)
- RKNN runtime library installed: `librknnrt.so` ‚úÖ
- 2.5G networking confirmed working on kernel 6.18 (previous issues were switch hardware fault)

**Completed:**
- [x] Updated `nixos/profiles/hw-orangepi5plus.nix` to kernel 6.18
- [x] Enabled RKNN module in Orange Pi hardware profile
- [x] Deployed to opi03 (testing), verified NPU detection
- [x] Confirmed `/dev/accel/accel0` device accessible
- [x] RKNN runtime library operational
- [x] Integrated RKNN overlay and module into NixOS configuration

#### 2.1 Integrate into Hardware Profile
- [x] Enabled RKNN module in `nixos/profiles/hw-orangepi5plus.nix`
- [x] Device permissions configured via udev rules
- [x] NPU device (`/dev/accel/accel0`) accessible to render group
- [x] Module auto-loads on aarch64 Orange Pi hosts

#### 2.2 Test on Hardware
- [x] Deployed to opi03 node with kernel 6.18
- [x] Verified RKNN runtime loads correctly
- [x] Confirmed rocket driver initializes all 3 NPU cores
- [x] Validated device node creation and permissions
- [ ] Test basic inference (ResNet-18 example from EZRKNN-Toolkit2) - TODO
- [ ] Benchmark inference performance - TODO
- [ ] Validate NPU core selection and utilization - TODO

### Phase 3: Kubernetes Integration

**Objective**: Enable K8s workloads to use the NPU

**STATUS**: ‚è≥ PENDING - After Phase 2

#### 3.1 Research NPU Device Access
- [ ] Investigate Kubernetes device plugin architecture for DRM accel devices
- [ ] Determine `/dev/drm/accel` exposure strategy
  - Option A: Device plugin for managed access
  - Option B: Privileged containers with device mounts
  - Option C: Hybrid approach
- [ ] Plan security model and access control

#### 3.2 Create Example Workload
- [ ] Build sample RKNN inference service
  - Image classification (ResNet-18)
  - Or object detection (YOLO)
- [ ] Deploy as K8s Deployment/Pod
- [ ] Document model conversion workflow
- [ ] Demonstrate end-to-end: model ‚Üí Kubernetes ‚Üí NPU inference

### Phase 4: Documentation

**Objective**: Document for future reference and community contribution

**STATUS**: ‚è≥ PENDING - Ongoing

#### 4.1 Document Usage
- [ ] Write "RKNN Module Usage Guide"
  - How to enable for a node/profile (once kernel available)
  - Configuration options
  - Dependency management
- [ ] Document model conversion workflow
  - Supported input formats (ONNX, TensorFlow, PyTorch, etc.)
  - Conversion tools and process
  - Optimization strategies
- [ ] Write "Kubernetes RKNN Workloads" guide
  - How to build RKNN inference apps
  - Device access from containers
  - Performance tuning
- [ ] Create troubleshooting guide
  - Common issues and solutions
  - Runtime version mismatches
  - Device access problems

#### 4.2 Optional Community Contribution
- [ ] Consider contributing RKNN packages to nixpkgs
- [ ] Share module patterns with NixOS community if beneficial

## Technical Details

### Hardware Verification (opi03 - 2025-12-10)

Verification commands and results:

```bash
# Check kernel version
$ ssh opi03.internal 'uname -r'
6.18.0

# Verify rocket driver is loaded
$ ssh opi03.internal 'lsmod | grep rocket'
rocket                 40960  0
gpu_sched              61440  2 rocket,panthor

# Check NPU device
$ ssh opi03.internal 'ls -la /dev/accel/accel0'
crw-rw-rw- 1 root render 261, 0 Dec 10 07:47 /dev/accel/accel0

# Verify NPU cores detected (requires sudo)
$ ssh opi03.internal 'sudo dmesg | grep "rocket.*npu"'
[   16.974889] rocket fdab0000.npu: Rockchip NPU core 0 version: 1179210309
[   16.991713] rocket fdac0000.npu: Rockchip NPU core 1 version: 1179210309
[   17.005136] rocket fdad0000.npu: Rockchip NPU core 2 version: 1179210309

# Verify RKNN runtime library
$ ssh opi03.internal 'ls -la /run/current-system/sw/lib/librknnrt.so'
lrwxrwxrwx 1 root root 79 Dec 31 1969 /run/current-system/sw/lib/librknnrt.so -> /nix/store/01idxnjagcl862f0wa6hlnf6dapjhwwl-rknn-runtime-2.3.2/lib/librknnrt.so
```

**Result**: All 3 NPU cores operational, runtime library installed, device accessible.

### RKNPU Kernel Driver History

**linux 6.17 and earlier**: No RKNPU support
- Submitted to kernel mailing list: June 2024
- Target: linux 6.18+
- Framework: DRM accelerator (DRM_ACCEL)

**linux 6.18-rc5** (current as of 2025-11-09):
- Driver: `drivers/accel/rocket/` - fully implemented
- Config: `DRM_ACCEL_ROCKET` (tristate: y/m/n)
- Dependencies: `ARCH_ROCKCHIP`, `ARM64`, `ROCKCHIP_IOMMU`, `MMU`
- Device exposure: `/dev/drm/accel/accel*` (via DRM framework)
- Userspace API: `include/uapi/drm/rocket_accel.h`
- Integration: Used by Mesa3D Rocket userspace driver

### Supported Model Formats (via RKNN Toolkit)

Conversion support (on desktop/PC):
- TensorFlow / TensorFlow Lite
- PyTorch (via ONNX)
- ONNX
- Caffe
- Darknet

Output: Quantized `.rknn` model files optimized for RK3588

### Inference APIs

**Python** (RKNN-Toolkit-Lite2):
```python
from rknnlite.api import RKNNLite

rknn = RKNNLite()
rknn.load_rknn('model.rknn')
rknn.init_runtime(core_mask=RKNNLite.NPU_CORE_0)
outputs = rknn.inference(inputs=[image_data])
rknn.release()
```

**C/C++**: RKNN Runtime APIs available via `librknnrt.so`

### Performance Characteristics

- ResNet-18 inference: ~71ms on RK3588 NPU
- 10-100x faster than CPU-only inference
- Energy efficient compared to CPU computation
- NPU cores can be selected (core_mask parameter)

## Success Criteria

- [x] RKNN module is reproducible and maintainable in Nix ‚úÖ
- [x] Kernel 6.18+ is available in nixpkgs with DRM_ACCEL_ROCKET enabled ‚úÖ
- [x] opi01-03 nodes detect NPU hardware and load rocket driver ‚úÖ
- [ ] opi01-03 nodes can run RKNN model inference (basic testing)
- [ ] Kubernetes pods can access NPU hardware
- [ ] Documentation enables others to build RKNN workloads
- [ ] Example workload demonstrates realistic usage

## Timeline

No specific deadline. Phases can be pursued at own pace:
- Phase 1: Foundation (‚úÖ COMPLETE - 2025-11-15)
- Phase 2: Hardware validation (‚úÖ COMPLETE - 2025-12-10)
- Phase 3: K8s integration (‚è≥ NEXT - pending inference testing)
- Phase 4: Documentation (üìù ONGOING - updated 2025-12-10)

## Related Issues & References

- EZRKNN-Toolkit2: https://github.com/Pelochus/EZRKNN-Toolkit2
- Official RKNN-Toolkit2: https://github.com/airockchip/rknn-toolkit2
- Rockchip RK3588 specs: Orange Pi 5 Plus hardware
- CLAUDE.md: Avalanche architecture and deployment patterns
- Kernel source: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git
  - Branch: v6.18-rc5 (has DRM_ACCEL_ROCKET)
  - Branch: v6.17 (no RKNPU support)

## Notes

- This is exploratory; no immediate production use cases
- NPU support is different from Ollama (LLM inference) - focused on computer vision and general ML
- Kernel 6.18.0 deployed successfully with all 3 NPU cores working
- Mainline `rocket` driver provides DRM_ACCEL framework integration
- Device accessible at `/dev/accel/accel0` with proper permissions
- RKNN runtime library (`librknnrt.so`) installed and available
- Python Toolkit Lite package built but not yet in system PATH (TODO)
- Next actions: Test basic inference, then explore Kubernetes integration
